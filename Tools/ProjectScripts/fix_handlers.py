#!/usr/bin/env python3
"""
Fix Handlers - AI-powered fix implementations for workflow recovery.

Extracted from ai_workflow_recovery.py for modularity.
Contains fix implementations for various error types:
- Python syntax errors
- Import errors
- Missing files
- Dependency errors
"""

import logging
import re
import subprocess
from pathlib import Path
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from .ai_workflow_recovery import WorkflowFailure

logger = logging.getLogger(__name__)


def fix_python_syntax(repo_path: Path, failure: "WorkflowFailure") -> bool:
    """Fix Python syntax errors."""
    logger.info("üêç Fixing Python syntax errors...")

    # Extract file and line from error
    match = re.search(r'File "([^"]+)", line (\d+)', failure.error_message)
    if not match:
        match = re.search(r"([^:]+):(\d+):", failure.error_message)

    if match:
        file_path = repo_path / match.group(1).lstrip("./")
        line_num = int(match.group(2))

        if file_path.exists():
            with open(file_path, "r") as f:
                lines = f.readlines()

            if line_num <= len(lines):
                line = lines[line_num - 1]

                # Fix unclosed strings
                if "EOL while scanning" in failure.error_message:
                    if line.count('"') % 2 == 1:
                        lines[line_num - 1] = line.rstrip() + '"\n'
                    elif line.count("'") % 2 == 1:
                        lines[line_num - 1] = line.rstrip() + "'\n"

                with open(file_path, "w") as f:
                    f.writelines(lines)

                logger.info(f"‚úÖ Fixed syntax in {file_path}:{line_num}")
                return True

    return False


def fix_imports(repo_path: Path, failure: "WorkflowFailure") -> bool:
    """Fix import-related errors."""
    logger.info("üì¶ Fixing import errors...")

    result = subprocess.run(
        ["flake8", "--select=F401", "."],
        capture_output=True,
        text=True,
        cwd=repo_path,
    )

    if result.returncode == 0 and not result.stdout:
        return True

    for line in result.stdout.split("\n"):
        if "F401" in line:
            match = re.match(
                r"([^:]+):(\d+):(\d+): F401 \'([^\']+)\' imported but unused", line
            )
            if match:
                file_path = repo_path / match.group(1)
                line_num = int(match.group(2))
                import_name = match.group(4)

                if file_path.exists():
                    with open(file_path, "r") as f:
                        lines = f.readlines()

                    if line_num <= len(lines):
                        import_line = lines[line_num - 1]
                        if (
                            f"import {import_name}" in import_line
                            or f'from {import_name.split(".")[0]}' in import_line
                        ):
                            lines[line_num - 1] = ""

                            with open(file_path, "w") as f:
                                f.writelines(lines)

                            logger.info(
                                f"‚úÖ Removed unused import: {import_name} from {file_path}"
                            )

    return True


def create_missing_file(repo_path: Path, failure: "WorkflowFailure") -> bool:
    """Create missing files based on error analysis."""
    logger.info("üìÑ Creating missing files...")

    match = re.search(
        r"No such file or directory: [\'\"]*([^\'\"]+)[\'\"]*", failure.error_message
    )
    if match:
        missing_file = match.group(1)
        file_path = repo_path / missing_file

        file_path.parent.mkdir(parents=True, exist_ok=True)

        if missing_file.endswith(".py"):
            content = '#!/usr/bin/env python3\n"""Auto-generated file by AI Workflow Recovery"""\npass\n'
        elif missing_file.endswith(".txt"):
            content = "# Auto-generated by AI Workflow Recovery\n"
        elif missing_file.endswith(".json"):
            content = "{}\n"
        else:
            content = "# Auto-generated by AI Workflow Recovery\n"

        with open(file_path, "w") as f:
            f.write(content)

        logger.info(f"‚úÖ Created missing file: {file_path}")
        return True

    return False


def fix_dependencies(repo_path: Path, failure: "WorkflowFailure") -> bool:
    """Fix dependency-related errors."""
    logger.info("üîó Fixing dependency errors...")

    req_file = repo_path / "requirements.txt"
    if not req_file.exists():
        basic_requirements = [
            "pytest>=7.0.0",
            "flake8>=5.0.0",
            "black>=23.0.0",
            "requests>=2.28.0",
            "pyyaml>=6.0",
        ]

        with open(req_file, "w") as f:
            f.write("\n".join(basic_requirements) + "\n")

        logger.info("‚úÖ Created requirements.txt with basic dependencies")
        return True

    return False


# Handler registry for easy lookup
FIX_HANDLERS = {
    "fix_python_syntax": fix_python_syntax,
    "fix_imports": fix_imports,
    "create_missing_file": create_missing_file,
    "fix_dependencies": fix_dependencies,
}


def apply_fix(repo_path: Path, failure: "WorkflowFailure") -> bool:
    """Apply the appropriate fix based on failure type."""
    handler = FIX_HANDLERS.get(failure.suggested_fix)
    if handler:
        try:
            return handler(repo_path, failure)
        except Exception as e:
            logger.error(f"Fix application failed: {e}")
            return False
    else:
        logger.warning(f"Unknown fix type: {failure.suggested_fix}")
        return False
